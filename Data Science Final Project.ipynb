{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import logging\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:yellow;\"> Scraping our data</h1>\n",
    "<h3> First thing we need to do to start the project would be to gather our data</h3>\n",
    "<p> We would like to find a reliable website of a very known fighting organization , this way we can trust the data since it is an official fighting organization </p>\n",
    "<p>We are going to scrape ESPN official website for getting our data </p>\n",
    "<a href=\"http://www.espn.com/mma/fighters\">  Link to ESPN's website where we can observe the fighter list </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = {'User-agent': 'Mozilla/5.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> First we would like to have a list with links for all pages of of the fither list </h3>\n",
    "<h3> first page will contain fithers with name started with letter A</h3>\n",
    "<h3> first page will contain fithers with name started with letter Z</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1=\"http://www.espn.com/mma/fighters\"\n",
    "response1 = requests.get(url1,headers=user_agent)\n",
    "pagesAToZ=[]\n",
    "# we would like to add the first page (the page that contains fighters with letter A)\n",
    "pagesAToZ.append(url1)\n",
    "soup1 = BeautifulSoup(response1.content, \"html.parser\")\n",
    "div_t=soup1.find(\"div\",attrs={\"class\":\"mod-content\"})\n",
    "hred_t=div_t.findAll(\"a\")\n",
    "for t in hred_t :\n",
    "    pagesAToZ.append(url1+t['href'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Let's print the urls so we can make sure everything looks fine </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.espn.com/mma/fighters', 'http://www.espn.com/mma/fighters?search=b', 'http://www.espn.com/mma/fighters?search=c', 'http://www.espn.com/mma/fighters?search=d', 'http://www.espn.com/mma/fighters?search=e', 'http://www.espn.com/mma/fighters?search=f', 'http://www.espn.com/mma/fighters?search=g', 'http://www.espn.com/mma/fighters?search=h', 'http://www.espn.com/mma/fighters?search=i', 'http://www.espn.com/mma/fighters?search=j', 'http://www.espn.com/mma/fighters?search=k', 'http://www.espn.com/mma/fighters?search=l', 'http://www.espn.com/mma/fighters?search=m', 'http://www.espn.com/mma/fighters?search=n', 'http://www.espn.com/mma/fighters?search=o', 'http://www.espn.com/mma/fighters?search=p', 'http://www.espn.com/mma/fighters?search=q', 'http://www.espn.com/mma/fighters?search=r', 'http://www.espn.com/mma/fighters?search=s', 'http://www.espn.com/mma/fighters?search=t', 'http://www.espn.com/mma/fighters?search=u', 'http://www.espn.com/mma/fighters?search=v', 'http://www.espn.com/mma/fighters?search=w', 'http://www.espn.com/mma/fighters?search=x', 'http://www.espn.com/mma/fighters?search=y', 'http://www.espn.com/mma/fighters?search=z']\n"
     ]
    }
   ],
   "source": [
    "print(pagesAToZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Another function we would like to writw is a functions that scrapes the most inner page that contain previews fights stats , we would like to get a path as an argument</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeinnerPageForFightStats(path):\n",
    "    \n",
    "    wins =\"\"\n",
    "    loses =\"\"\n",
    "    decision = \"\"\n",
    "    winByKo = \"\"\n",
    "    loseByKo= \"\"\n",
    "    winbySubmission=\"\"\n",
    "    loseBySubmission=\"\"\n",
    "    page_link='http://www.espn.com'+path\n",
    "   #  print(\"pagelink is\" , page_link )\n",
    "    response= requests.get(page_link)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    regex_winsLosesDecision = \"W-L-D(\\d)-(\\d)-(\\d)\"\n",
    "    regex_ko=\"\\(T\\)KO(\\d)-(\\d)\"\n",
    "    regex_sub=\"SUB(\\d)-(\\d)\"\n",
    "    ul_t = soup.find(\"ul\", attrs={\"class\":\"StatBlock__Content flex list ph4 pv3 justify-between\"}) \n",
    "    if(ul_t==None):\n",
    "       print(\"No unordered list atrr\")\n",
    "\n",
    "    if( (not hasattr(ul_t,'findAll')) ):\n",
    "      return wins , loses , decision , winByKo , loseByKo, winbySubmission , loseBySubmission\n",
    "\n",
    "   \n",
    "    if( not ul_t.findAll(\"li\") ):\n",
    "      return wins , loses , decision , winByKo , loseByKo, winbySubmission , loseBySubmission\n",
    "        \n",
    "    li_list=ul_t.findAll(\"li\")\n",
    "    for li in li_list:\n",
    "       text=li.text\n",
    "    #    print(text)\n",
    "       winLoseDecStats_regex_res=re.search(regex_winsLosesDecision , text)\n",
    "       koStats_regex_res=re.search(regex_ko , text)\n",
    "       subStats_regex_res=re.search(regex_sub , text)\n",
    "       if(winLoseDecStats_regex_res):\n",
    "           wins=winLoseDecStats_regex_res.group(1)\n",
    "           loses=winLoseDecStats_regex_res.group(2)\n",
    "           decision=winLoseDecStats_regex_res.group(3)\n",
    "        #    print(winLoseDecStats_regex_res)\n",
    "       elif(koStats_regex_res):\n",
    "           winByKo=koStats_regex_res.group(1)\n",
    "           loseByKo=koStats_regex_res.group(2)\n",
    "        #    print(koStats_regex_res)\n",
    "       elif(subStats_regex_res):\n",
    "           winbySubmission=subStats_regex_res.group(1)\n",
    "           loseBySubmission=subStats_regex_res.group(2)\n",
    "        #    print(subStats_regex_res)\n",
    "    logging.info(wins,[wins , loses , decision , winByKo , loseByKo, winbySubmission , loseBySubmission])\n",
    "    print(wins , loses , decision , winByKo , loseByKo, winbySubmission , loseBySubmission)\n",
    "    return wins , loses , decision , winByKo , loseByKo, winbySubmission , loseBySubmission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's make sure we get the right return params and that the regexes are fine</h3>\n",
    "<p> we will expect three matches and output params ('0', '1', '0', '0', '0', '0', '1') </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapeinnerPageForFightStats('/mma/fighter/_/id/3043549/niina-aaltonen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Cool , We have a list of links to each page we want to scrape and the stats from the inner pages! </h3>\n",
    "<h4>Now we would like to build a function for scraping each page  </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "/The structure of the page containes a long table with two columns . <br>\n",
    "First Column - Name <br>\n",
    "Sec Column - Country <br>\n",
    "\n",
    "the Name is clickable and redirect us to another page with more details of the fighter\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableRows(page_link):\n",
    "    response= requests.get(page_link)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    table = soup.find(lambda tag: tag.name=='table') \n",
    "    rows = table.findAll(lambda tag: tag.name=='tr')\n",
    "    # we would like to remove the first two rows bc it doesnt contain any data\n",
    "    del rows[0]\n",
    "    del rows[0]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runOnSingleDataRowGetNameCountryBio(row):\n",
    "    #currently runs only on the first row\n",
    "        full_name , link_to_bio , country = \"\" , \"\" ,\"\"\n",
    "        cells = row.findAll(lambda tag: tag.name=='td')\n",
    "        if(cells == None or (not cells)):\n",
    "            return \n",
    "        full_name = cells[0].find(\"a\").text\n",
    "        full_name=full_name.split(\", \")\n",
    "        first_name=full_name[0]\n",
    "        last_name=full_name[1]\n",
    "        link_to_bio = cells[0].find(\"a\")[\"href\"]\n",
    "        country=cells[1].text\n",
    "        full_name=first_name+\" \"+last_name \n",
    "        return full_name , link_to_bio ,country\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeTheWholePage(page_url):\n",
    "    data=[]\n",
    "    rows=getTableRows(page_url)\n",
    "    for row in rows:\n",
    "        full_name  , bio_link , country = runOnSingleDataRowGetNameCountryBio(row)\n",
    "        wins , loses , decision , winByKo , loseByKo, winbySubmission , loseBySubmission =scrapeinnerPageForFightStats(bio_link)\n",
    "        # print(full_name , bio_link , country)\n",
    "        # print(wins , loses , decision , winByKo , loseByKo, winbySubmission , loseBySubmission)\n",
    "        listOfParams=[full_name  , country , wins , loses , decision , winByKo , loseByKo, winbySubmission , loseBySubmission ]\n",
    "        # print(listOfParams)\n",
    "        print(full_name)\n",
    "        logging.info(full_name)\n",
    "        data.append(listOfParams)\n",
    "        # now we need to return DF with the whole page data\n",
    "        #next were gonna gather all the dfs together and save it to a file\n",
    "    #return df\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['Full_Name', 'Country', 'Wins' , 'Loses' ,'Decision' , 'Wins_By_KO' , 'Loses_By_KO', 'Wins_By_Submission' , 'Loses_By_Submission' ])\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> test ourselves on the main page </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapeTheWholePage('http://www.espn.com/mma/fighters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now we need to orginize our data in a csv file  , since we have lots of data , we will split it to a few files</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframToCsvFile(df , file_name, path):\n",
    "    os.makedirs(path, exist_ok=True) \n",
    "    df.to_csv(path+file_name+'.csv' , index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapePageAndSaveToCsv(page_link , page_num):\n",
    "        df=scrapeTheWholePage(page_link)\n",
    "        dataframToCsvFile(df,'dataPage'+str(page_num))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> We're going to use threads to scrape all pages in pharllel </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrapeMultiPagesAnd(pagesToScrap):\n",
    "    threadList=[]\n",
    "    for i in range(len(pagesToScrap)):\n",
    "             threadObj=threading.Thread(target=scrapePageAndSaveToCsv, args=(pagesToScrap[i],i+6,))\n",
    "             threadObj.start();\n",
    "             threadList.append(threadObj)\n",
    "    for t in threadList:\n",
    "             t.join();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagesToScrap=pagesAToZ\n",
    "# scrapeMultiPagesAnd(pagesToScrap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Perfect ! now we have all our data scraped and reay for the next step 🥳 🥳 </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 > Next Step <span style=\"color:green;\"> Cleaning our Data 🧽 🧹<span> </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "    return pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full_Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Loses</th>\n",
       "      <th>Decision</th>\n",
       "      <th>Wins_By_KO</th>\n",
       "      <th>Loses_By_KO</th>\n",
       "      <th>Wins_By_Submission</th>\n",
       "      <th>Loses_By_Submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zaba Arkadiusz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zaba Marcin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zabolotny Dmitry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zabrocki Michael</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zac Charlie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zachrich Luke</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Zaczkiewicz Sylwia</td>\n",
       "      <td>POL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Zadernovsky Igor</td>\n",
       "      <td>POL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Zadruzynski Filip</td>\n",
       "      <td>POL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zafir Anton</td>\n",
       "      <td>AUS</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Full_Name Country  Wins  Loses  Decision  Wins_By_KO  Loses_By_KO  \\\n",
       "0      Zaba Arkadiusz     NaN   0.0    1.0       0.0         0.0          0.0   \n",
       "1         Zaba Marcin     NaN   0.0    1.0       0.0         0.0          0.0   \n",
       "2    Zabolotny Dmitry     NaN   1.0    3.0       0.0         0.0          2.0   \n",
       "3    Zabrocki Michael     NaN   0.0    1.0       0.0         0.0          1.0   \n",
       "4         Zac Charlie     NaN   0.0    2.0       0.0         0.0          0.0   \n",
       "5       Zachrich Luke     USA   NaN    NaN       NaN         4.0          1.0   \n",
       "6  Zaczkiewicz Sylwia     POL   0.0    1.0       0.0         0.0          1.0   \n",
       "7    Zadernovsky Igor     POL   0.0    2.0       0.0         0.0          0.0   \n",
       "8   Zadruzynski Filip     POL   0.0    1.0       0.0         0.0          0.0   \n",
       "9         Zafir Anton     AUS   7.0    3.0       0.0         3.0          3.0   \n",
       "\n",
       "   Wins_By_Submission  Loses_By_Submission  \n",
       "0                 0.0                  0.0  \n",
       "1                 0.0                  0.0  \n",
       "2                 1.0                  1.0  \n",
       "3                 0.0                  0.0  \n",
       "4                 0.0                  2.0  \n",
       "5                 7.0                  3.0  \n",
       "6                 0.0                  0.0  \n",
       "7                 0.0                  2.0  \n",
       "8                 0.0                  1.0  \n",
       "9                 3.0                  0.0  "
      ]
     },
     "execution_count": 803,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=load_csv('./scrapedData/dataPage25.csv')\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillNansWihtValues(df , col_names_list , value_to_fill_with):\n",
    "    df_cpy=df.copy();\n",
    "    for col_name in col_names_list:\n",
    "         df_cpy[col_name]=df_cpy[col_name].fillna(value_to_fill_with).copy()\n",
    "    return df_cpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeColFromDf(df , cols_list_to_remove):\n",
    "    df_cpy=df.copy()\n",
    "    return df_cpy.drop(cols_list_to_remove , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addColToDf(df , col_name , col_fixed_value):\n",
    "    df_cpy=df.copy()\n",
    "    df_cpy[col_name]=col_fixed_value\n",
    "    return df_cpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def checkNameAndChangeChampionVal(df , list_of_champions):\n",
    "    # for each row  -> if name in list_of_champions ->isChamp= 1\n",
    "    # df = df.reset_index()\n",
    "    for index,row in df.iterrows():\n",
    "        if row['Full_Name'] in list_of_champions :\n",
    "            row['Champion']=1\n",
    "            print(\"  Found Champion  \")\n",
    "            print(\" Index = \" , index)\n",
    "            print(\"Name = \" , row['Full_Name'])\n",
    "            print(\"changes Champion Val from 0 to \",row['Champion'] )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows_with_NA_vals_in_specific_col(df , specific_col ):\n",
    "    df_cpy=df.copy()\n",
    "    return df_cpy.dropna(subset=[specific_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df, col):\n",
    "    df_cpy=df.copy()\n",
    "    df_cpy=df_cpy.drop_duplicates(subset=[col])\n",
    "    return df_cpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleanUpFlow(out_folder_loc , list_of_file_names):\n",
    "    for file in list_of_file_names:\n",
    "        scrapedData_filename='./scrapedData/'+file+'.csv'\n",
    "\n",
    "        df=load_csv(scrapedData_filename)\n",
    "        df=remove_duplicates(df , 'Full_Name')\n",
    "        df=remove_rows_with_NA_vals_in_specific_col(df , 'Country' )\n",
    "        df=remove_rows_with_NA_vals_in_specific_col(df , 'Full_Name' )\n",
    "        df=fillNansWihtValues(df , ['Decision' , 'Wins_By_KO' , 'Loses_By_KO' , 'Wins_By_Submission','Loses_By_Submission'] , 0)\n",
    "        df=removeColFromDf(df , ['Wins' , 'Loses'])\n",
    "        df=addColToDf(df , 'Champion' , 0)\n",
    "        dataframToCsvFile(df,file+'.csv' , out_folder_loc)\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkNameAndChangeChampionVal(df , list_of_champions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfFiles=['dataPage'+str(i)  for i in range(26)]\n",
    "path='./cleanedData/'\n",
    "dataCleanUpFlow(path,listOfFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
